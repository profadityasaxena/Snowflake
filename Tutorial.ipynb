{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb24857c",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction to Snowflake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0cdec3",
   "metadata": {},
   "source": [
    "---\n",
    "### Chapter 1: Introduction to Snowflake\n",
    "\n",
    "#### 1.1 Overview\n",
    "\n",
    "Snowflake is a cloud-native data platform that redefines traditional data warehousing and analytics through a fully managed, multi-cluster, shared data architecture. Unlike conventional on-premises systems, Snowflake decouples storage and compute resources, allowing for elastic scalability, high concurrency, and cost-efficient usage across diverse workloads including SQL analytics, machine learning, and real-time data applications. Built from the ground up for cloud environments like Amazon Web Services, Microsoft Azure, and Google Cloud Platform, Snowflake eliminates the complexities of infrastructure management while ensuring secure, performant, and ACID-compliant data operations.\n",
    "\n",
    "#### 1.2 Architectural Design\n",
    "\n",
    "Snowflakeâ€™s architecture consists of three independently scalable layers: **storage**, **compute**, and **cloud services**. The storage layer provides centralized, compressed, and optimized columnar storage. The compute layer offers isolated virtual warehouses that process queries without resource contention. The cloud services layer handles metadata management, query parsing, access control, and optimization logic. This separation enables Snowflake to deliver high availability, workload isolation, and seamless scaling for concurrent users and applications.\n",
    "\n",
    "#### 1.3 Key Platform Capabilities\n",
    "\n",
    "Snowflakeâ€™s native support for semi-structured data formats such as JSON, Avro, and Parquet empowers organizations to analyze diverse data types using familiar SQL. Features like **Time Travel** and **Fail-safe** allow users to access historical versions of data and recover from accidental modifications. **Zero-copy cloning** and **data sharing** enable efficient data reuse and collaboration across teams and even organizations, without physically duplicating data.\n",
    "\n",
    "#### 1.4 Multi-Cloud Strategy\n",
    "\n",
    "Snowflake is deployed natively across all major cloud providers â€” Amazon Web Services, Microsoft Azure, and Google Cloud Platform â€” offering flexibility in infrastructure choice while maintaining a consistent user experience. This multi-cloud design enables customers to adopt hybrid strategies, ensure business continuity, and meet regional compliance requirements. Cross-cloud replication and global data sharing features further reinforce its enterprise-grade capabilities.\n",
    "\n",
    "#### 1.5 Role in the Modern Data Stack\n",
    "\n",
    "As a foundational component of the modern data stack, Snowflake bridges the gap between data engineering, business intelligence, and advanced analytics. It integrates seamlessly with tools for ELT (e.g., dbt, Fivetran), BI (e.g., Power BI, Tableau), and machine learning (e.g., DataRobot, H2O.ai). Its role extends from operational analytics to data science and governance, making it a catalyst for digital transformation across industries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eacdba",
   "metadata": {},
   "source": [
    "---\n",
    "### Chapter 2: Stages and Data Ingestion in Snowflake\n",
    "\n",
    "\n",
    "#### 2.1 What Are Stages in Snowflake?\n",
    "\n",
    "In Snowflake, a **stage** is a temporary or permanent location used to hold data files prior to loading them into a table or after unloading data from a table. Stages abstract the file storage layer from the compute layer, enabling scalable, fault-tolerant ingestion of batch and streaming data. Stages serve as the intermediate point between external data sources and Snowflakeâ€™s analytical engine, allowing Snowflake to process structured and semi-structured data (CSV, JSON, Avro, Parquet, ORC, etc.) using robust SQL-based operations.\n",
    "\n",
    "#### 2.2 Purpose of Stages\n",
    "\n",
    "The primary purposes of stages in Snowflake are:\n",
    "\n",
    "- **Data Ingestion:** Upload files to a stage before loading them into tables using the `COPY INTO` command.\n",
    "- **Decoupling Compute from Storage:** Avoid direct table writes during ingestion, improving efficiency and fault isolation.\n",
    "- **Temporary Storage:** Use temporary stages during ETL processes or for short-lived operations.\n",
    "- **Data Export:** Export table data to a stage using the `COPY INTO` command in reverse.\n",
    "- **Integration:** Enable connectors, scripts, and external tools to interact with Snowflake asynchronously.\n",
    "\n",
    "Stages make Snowflakeâ€™s ingestion pipeline more modular and scalable, especially in multi-tenant or multi-cloud environments.\n",
    "\n",
    "#### 2.3 Types of Stages in Snowflake\n",
    "\n",
    "Snowflake supports **three types of stages**, each with a specific use case:\n",
    "\n",
    "##### 1. User Stages\n",
    "- Automatically created for every Snowflake user.\n",
    "- Named as `@~`\n",
    "- Only accessible by the specific user.\n",
    "- Typically used for small, ad-hoc file uploads during testing or experimentation.\n",
    "\n",
    "##### 2. Table Stages\n",
    "- Automatically created for every Snowflake table.\n",
    "- Named as `@%<table_name>`\n",
    "- Used for loading data into or unloading data from a specific table.\n",
    "- Ensures tight coupling of the data and its schema context.\n",
    "\n",
    "##### 3. Named Stages\n",
    "- Manually created using the `CREATE STAGE` command.\n",
    "- Can be **internal** (hosted within Snowflake) or **external** (linked to AWS S3, Azure Blob, or GCP Cloud Storage).\n",
    "- Ideal for production pipelines and large-scale ETL operations.\n",
    "\n",
    "Each type offers different levels of **visibility, security, lifecycle management**, and **integration flexibility**.\n",
    "\n",
    "#### 2.4 Data Ingestion Workflow\n",
    "\n",
    "Data ingestion into Snowflake typically follows these steps:\n",
    "\n",
    "1. **Upload files to a stage**  \n",
    "   - Use `PUT` command for internal stages  \n",
    "   - Use Snowpipe, SDKs, or cloud-native triggers for external stages\n",
    "\n",
    "2. **Preview staged files**  \n",
    "   - Use `LIST @stage_name` to confirm successful uploads\n",
    "\n",
    "3. **Load into tables**  \n",
    "   - Use the `COPY INTO <table>` command to load data from the stage\n",
    "   - Apply file format settings (e.g., CSV delimiter, JSON path, etc.)\n",
    "\n",
    "4. **Post-load cleanup (optional)**  \n",
    "   - Remove staged files with `REMOVE @stage_name` if they are no longer needed\n",
    "\n",
    "5. **Automation (optional)**  \n",
    "   - Use **Snowpipe** for continuous loading from external stages using event-based triggers or REST APIs\n",
    "\n",
    "\n",
    "   #### ðŸ“Œ Extended Examples: Ingesting Data into Snowflake\n",
    "\n",
    "Below are several examples demonstrating how to ingest CSV data into Snowflake from both internal and external stages. Each example is annotated with comments and accompanied by explanatory text.\n",
    "\n",
    "---\n",
    "\n",
    "##### ðŸ”¹ Example 1: Upload and Load CSV via Internal Stage\n",
    "\n",
    "This is the most basic and direct approach to load data from a local machine into Snowflake using an internal stage.\n",
    "\n",
    "```sql\n",
    "-- Step 1: Upload a file from your local system to an internal stage\n",
    "-- Note: This command must be run from SnowSQL or another supported client.\n",
    "PUT file://local_path/mydata.csv @my_internal_stage;\n",
    "\n",
    "-- Step 2: Load the staged file into a Snowflake table\n",
    "COPY INTO my_table\n",
    "FROM @my_internal_stage\n",
    "FILE_FORMAT = (\n",
    "    TYPE = 'CSV'                     -- Specify that the source file is in CSV format\n",
    "    FIELD_OPTIONALLY_ENCLOSED_BY = '\"'  -- Handle values enclosed in double quotes\n",
    "    SKIP_HEADER = 1                 -- Skip the first row (header row)\n",
    ");\n",
    "\n",
    "\n",
    "\n",
    "##### ðŸ”¹ Example 2: Loading Multiple Files from a Named Internal Stage\n",
    "\n",
    "You can upload multiple files to a named internal stage and then bulk load them into a target table.\n",
    "\n",
    "-- Assume the files are already uploaded to the named stage: @my_stage\n",
    "-- You can use LIST to verify uploaded files:\n",
    "LIST @my_stage;\n",
    "\n",
    "-- Load all files in the stage into the destination table\n",
    "COPY INTO sales_data\n",
    "FROM @my_stage\n",
    "FILE_FORMAT = (\n",
    "    TYPE = 'CSV'\n",
    "    FIELD_DELIMITER = ','           -- Specify delimiter if not using default\n",
    "    SKIP_HEADER = 1\n",
    "    ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE  -- Useful if file column count is inconsistent\n",
    ");\n",
    "\n",
    "##### ðŸ”¹ Example 3: Ingesting from External Stage (e.g., AWS S3)\n",
    "\n",
    "To ingest from an external cloud location (e.g., Amazon S3), you must create an external stage and reference it in the COPY INTO command.\n",
    "\n",
    "-- Create the external stage referencing your S3 bucket\n",
    "CREATE OR REPLACE STAGE s3_stage\n",
    "URL = 's3://my-bucket-name/snowflake-data/'\n",
    "STORAGE_INTEGRATION = my_s3_integration;\n",
    "\n",
    "-- Load data from the external stage\n",
    "COPY INTO inventory_table\n",
    "FROM @s3_stage\n",
    "FILE_FORMAT = (\n",
    "    TYPE = 'CSV'\n",
    "    FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "    SKIP_HEADER = 1\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82455a90",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
